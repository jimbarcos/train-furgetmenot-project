[18:51:51] Data generators loaded from CSV files
[18:51:51] Welcome to Real-Time Model Training Interface!
[18:51:51] Select a model and configure parameters to begin training.
[18:52:20] Loading proposed model...
[18:52:23] Proposed Siamese CapsNet + MobileNetV2 loaded successfully.
[18:52:28] Reloading data generators with current batch size...
[18:52:41] Data generators loaded from CSV files
[18:52:41] Model directory created: models\Proposed Siamese CapsNet + MobileNetV2_20251007_185241
[18:52:41] Starting training for 50 epochs...
[18:52:41] Preparing model and data for training...
[18:52:43] [RoutingEntropy] mean_norm_entropy=0.9823
[18:52:45] Adaptive threshold mechanism enabled
[18:52:45] Batch sizes - Train: 32, Val: 16
[18:52:45] Data ready - Train: 625 batches, Val: 625 batches
[18:52:45] Training with learning rate: 5.00e-05
[18:52:45] Mixed precision training enabled
[18:52:45] Starting training with 50 epochs...
[18:52:45] Model parameters: 5,643,785
[18:52:45] [Pre-Train Distances] n=16 min=0.02743 max=0.16021 mean=0.06954 std=0.02951 spread=0.13278
[18:52:45] Models will be saved to: models\Proposed Siamese CapsNet + MobileNetV2_20251007_185241\models
[18:52:46] Training started - Preparing data...
[18:52:46] [ThresholdSync] Epoch 1: train threshold (dist) = 0.5000
[18:52:46] Starting Epoch 1...
[18:53:01]    Batch 0: Loss 1.0901
[18:53:30]    Batch 20: Loss 1.0720
[18:53:53]    Batch 40: Loss 1.0548
[18:54:16]    Batch 60: Loss 1.0392
[18:54:41]    Batch 80: Loss 1.0250
[18:55:04]    Batch 100: Loss 1.0116
[18:55:31]    Batch 120: Loss 0.9990
[18:55:57]    Batch 140: Loss 0.9866
[18:56:23]    Batch 160: Loss 0.9746
[18:56:50]    Batch 180: Loss 0.9631
[18:57:17]    Batch 200: Loss 0.9518
[18:57:42]    Batch 220: Loss 0.9407
[18:58:07]    Batch 240: Loss 0.9298
[18:58:31]    Batch 260: Loss 0.9192
[18:58:54]    Batch 280: Loss 0.9088
[18:59:17]    Batch 300: Loss 0.8986
[18:59:40]    Batch 320: Loss 0.8885
[19:00:03]    Batch 340: Loss 0.8786
[19:00:26]    Batch 360: Loss 0.8688
[19:00:49]    Batch 380: Loss 0.8593
[19:01:12]    Batch 400: Loss 0.8499
[19:01:35]    Batch 420: Loss 0.8406
[19:01:58]    Batch 440: Loss 0.8316
[19:02:22]    Batch 460: Loss 0.8226
[19:02:47]    Batch 480: Loss 0.8138
[19:03:11]    Batch 500: Loss 0.8052
[19:03:36]    Batch 520: Loss 0.7967
[19:04:02]    Batch 540: Loss 0.7883
[19:04:27]    Batch 560: Loss 0.7800
[19:04:53]    Batch 580: Loss 0.7719
[19:05:18]    Batch 600: Loss 0.7638
[19:05:46]    Batch 620: Loss 0.7560
[19:26:53] [Val Epoch 1] raw_min=0.0041 raw_max=0.9999 mode=dist->sim thr_score=0.650 thr_dist=0.441 f1=0.9110 auc=0.9646 acc=0.9110 edge_fb=0
[19:26:57] Epoch 1/50: loss=0.7544, val_loss=0.5188, acc=0.8132, val_acc=0.9110, val_f1=0.9110, val_auc=0.9646, thr=0.441, precision=0.8166, recall=0.8078 [2046.9s/epoch, ETA: 1671.8min]
[19:27:08] [ThresholdSync] Epoch 2: train threshold (dist) = 0.4410
[19:27:08] Starting Epoch 2...
[19:27:08]    Batch 0: Loss 0.5110
[19:27:27]    Batch 20: Loss 0.5039
[19:27:52]    Batch 40: Loss 0.4979
[19:28:18]    Batch 60: Loss 0.4922
[19:28:43]    Batch 80: Loss 0.4867
[19:29:07]    Batch 100: Loss 0.4812
[19:29:32]    Batch 120: Loss 0.4759
[19:29:59]    Batch 140: Loss 0.4706
[19:30:25]    Batch 160: Loss 0.4653
[19:30:51]    Batch 180: Loss 0.4601
[19:31:16]    Batch 200: Loss 0.4550
[19:31:42]    Batch 220: Loss 0.4500
[19:32:06]    Batch 240: Loss 0.4451
[19:32:34]    Batch 260: Loss 0.4401
[19:33:01]    Batch 280: Loss 0.4353
[19:33:26]    Batch 300: Loss 0.4306
[19:33:52]    Batch 320: Loss 0.4259
[19:34:17]    Batch 340: Loss 0.4214
[19:34:45]    Batch 360: Loss 0.4169
[19:35:10]    Batch 380: Loss 0.4125
[19:35:35]    Batch 400: Loss 0.4081
[19:36:00]    Batch 420: Loss 0.4038
[19:36:26]    Batch 440: Loss 0.3996
[19:36:53]    Batch 460: Loss 0.3954
[19:37:19]    Batch 480: Loss 0.3913
[19:37:44]    Batch 500: Loss 0.3873
[19:38:10]    Batch 520: Loss 0.3833
[19:38:36]    Batch 540: Loss 0.3794
[19:39:00]    Batch 560: Loss 0.3756
[19:39:25]    Batch 580: Loss 0.3718
[19:39:51]    Batch 600: Loss 0.3680
[19:40:16]    Batch 620: Loss 0.3644
[20:01:02] [Val Epoch 2] raw_min=0.0016 raw_max=0.9999 mode=dist->sim thr_score=0.590 thr_dist=0.429 f1=0.9246 auc=0.9680 acc=0.9245 edge_fb=0
[20:01:09] Epoch 2/50: loss=0.3636, val_loss=0.2575, acc=0.9102, val_acc=0.9245, val_f1=0.9246, val_auc=0.9680, thr=0.429, precision=0.9133, recall=0.9065 [2043.4s/epoch, ETA: 1638.5min]
[20:01:20] [ThresholdSync] Epoch 3: train threshold (dist) = 0.4290
[20:01:20] Starting Epoch 3...
[20:01:20]    Batch 0: Loss 0.2501
[20:01:35]    Batch 20: Loss 0.2464
[20:01:59]    Batch 40: Loss 0.2437
[20:02:22]    Batch 60: Loss 0.2412
[20:02:46]    Batch 80: Loss 0.2387
[20:03:10]    Batch 100: Loss 0.2363
[20:03:35]    Batch 120: Loss 0.2338
[20:03:58]    Batch 140: Loss 0.2314
[20:04:22]    Batch 160: Loss 0.2289
[20:04:47]    Batch 180: Loss 0.2265
[20:05:10]    Batch 200: Loss 0.2241
[20:05:34]    Batch 220: Loss 0.2218
[20:05:58]    Batch 240: Loss 0.2195
[20:06:22]    Batch 260: Loss 0.2172
[20:06:46]    Batch 280: Loss 0.2149
[20:07:10]    Batch 300: Loss 0.2127
[20:07:34]    Batch 320: Loss 0.2106
[20:07:59]    Batch 340: Loss 0.2085
[20:08:22]    Batch 360: Loss 0.2065
[20:08:46]    Batch 380: Loss 0.2045
[20:09:10]    Batch 400: Loss 0.2025
[20:09:35]    Batch 420: Loss 0.2005
[20:10:01]    Batch 440: Loss 0.1986
[20:10:27]    Batch 460: Loss 0.1967
[20:10:54]    Batch 480: Loss 0.1948
[20:11:19]    Batch 500: Loss 0.1929
[20:11:44]    Batch 520: Loss 0.1911
[20:12:09]    Batch 540: Loss 0.1893
[20:12:34]    Batch 560: Loss 0.1875
[20:13:00]    Batch 580: Loss 0.1858
[20:13:25]    Batch 600: Loss 0.1841
[20:13:50]    Batch 620: Loss 0.1824
[20:34:34] [Val Epoch 3] raw_min=0.0013 raw_max=0.9999 mode=dist->sim thr_score=0.570 thr_dist=0.430 f1=0.9420 auc=0.9758 acc=0.9415 edge_fb=0
[20:34:38] Epoch 3/50: loss=0.1821, val_loss=0.1380, acc=0.9347, val_acc=0.9415, val_f1=0.9420, val_auc=0.9758, thr=0.430, precision=0.9312, recall=0.9386 [2005.2s/epoch, ETA: 1594.9min]
[20:34:49] [ThresholdSync] Epoch 4: train threshold (dist) = 0.4297
[20:34:49] Starting Epoch 4...
[20:34:49]    Batch 0: Loss 0.1300
[20:35:07]    Batch 20: Loss 0.1287
[20:35:32]    Batch 40: Loss 0.1272
[20:35:58]    Batch 60: Loss 0.1259
[20:36:23]    Batch 80: Loss 0.1247
[20:36:48]    Batch 100: Loss 0.1236
[20:37:13]    Batch 120: Loss 0.1224
[20:37:39]    Batch 140: Loss 0.1212
[20:38:03]    Batch 160: Loss 0.1201
[20:38:28]    Batch 180: Loss 0.1190
[20:38:53]    Batch 200: Loss 0.1179
[20:39:18]    Batch 220: Loss 0.1168
[20:39:42]    Batch 240: Loss 0.1158
[20:40:08]    Batch 260: Loss 0.1148
[20:40:32]    Batch 280: Loss 0.1138
[20:40:57]    Batch 300: Loss 0.1128
[20:41:21]    Batch 320: Loss 0.1118
[20:41:46]    Batch 340: Loss 0.1108
[20:42:11]    Batch 360: Loss 0.1099
[20:42:36]    Batch 380: Loss 0.1089
[20:43:02]    Batch 400: Loss 0.1080
[20:43:28]    Batch 420: Loss 0.1071
[20:43:52]    Batch 440: Loss 0.1062
[20:44:19]    Batch 460: Loss 0.1054
[20:44:46]    Batch 480: Loss 0.1045
[20:45:09]    Batch 500: Loss 0.1037
[20:45:33]    Batch 520: Loss 0.1029
[20:45:58]    Batch 540: Loss 0.1020
[20:46:23]    Batch 560: Loss 0.1012
[20:46:48]    Batch 580: Loss 0.1004
[20:47:13]    Batch 600: Loss 0.0996
[20:47:38]    Batch 620: Loss 0.0989
[21:07:59] [Val Epoch 4] raw_min=0.0006 raw_max=0.9999 mode=dist->sim thr_score=0.640 thr_dist=0.402 f1=0.9405 auc=0.9795 acc=0.9405 edge_fb=0
[21:08:04] Epoch 4/50: loss=0.0987, val_loss=0.0828, acc=0.9488, val_acc=0.9405, val_f1=0.9405, val_auc=0.9795, thr=0.402, precision=0.9391, recall=0.9598 [2000.1s/epoch, ETA: 1555.1min]
[21:08:13] [ThresholdSync] Epoch 5: train threshold (dist) = 0.4019
[21:08:13] Starting Epoch 5...
[21:08:13]    Batch 0: Loss 0.0736
[21:08:29]    Batch 20: Loss 0.0736
[21:08:51]    Batch 40: Loss 0.0731
[21:09:13]    Batch 60: Loss 0.0725
[21:09:35]    Batch 80: Loss 0.0721
[21:09:57]    Batch 100: Loss 0.0715
[21:10:18]    Batch 120: Loss 0.0710
[21:10:40]    Batch 140: Loss 0.0705
[21:11:02]    Batch 160: Loss 0.0699
[21:11:24]    Batch 180: Loss 0.0694
[21:11:46]    Batch 200: Loss 0.0689
[21:12:08]    Batch 220: Loss 0.0684
[21:12:30]    Batch 240: Loss 0.0680
[21:12:52]    Batch 260: Loss 0.0675
[21:13:14]    Batch 280: Loss 0.0670
[21:13:36]    Batch 300: Loss 0.0665
[21:13:58]    Batch 320: Loss 0.0660
[21:14:20]    Batch 340: Loss 0.0656
[21:14:42]    Batch 360: Loss 0.0651
[21:15:04]    Batch 380: Loss 0.0647
[21:15:27]    Batch 400: Loss 0.0642
[21:15:49]    Batch 420: Loss 0.0638
[21:16:11]    Batch 440: Loss 0.0634
[21:16:33]    Batch 460: Loss 0.0629
[21:16:54]    Batch 480: Loss 0.0625
[21:17:16]    Batch 500: Loss 0.0621
[21:17:38]    Batch 520: Loss 0.0617
[21:18:00]    Batch 540: Loss 0.0613
[21:18:22]    Batch 560: Loss 0.0609
[21:18:44]    Batch 580: Loss 0.0605
[21:19:06]    Batch 600: Loss 0.0601
[21:19:28]    Batch 620: Loss 0.0597
[21:38:46] [Val Epoch 5] raw_min=0.0004 raw_max=0.9999 mode=dist->sim thr_score=0.680 thr_dist=0.369 f1=0.9409 auc=0.9781 acc=0.9410 edge_fb=0
[21:38:53] Epoch 5/50: loss=0.0597, val_loss=0.0563, acc=0.9527, val_acc=0.9410, val_f1=0.9409, val_auc=0.9781, thr=0.369, precision=0.9421, recall=0.9647 [1841.1s/epoch, ETA: 1494.0min]
[21:39:05] [ThresholdSync] Epoch 6: train threshold (dist) = 0.3693
[21:39:05] Starting Epoch 6...
[21:39:05]    Batch 0: Loss 0.0461
[21:39:15]    Batch 20: Loss 0.0465
[21:39:36]    Batch 40: Loss 0.0467
[21:39:58]    Batch 60: Loss 0.0465
[21:40:19]    Batch 80: Loss 0.0462
[21:40:41]    Batch 100: Loss 0.0459
[21:41:02]    Batch 120: Loss 0.0456
[21:41:24]    Batch 140: Loss 0.0453
[21:41:45]    Batch 160: Loss 0.0450
[21:42:07]    Batch 180: Loss 0.0448
[21:42:28]    Batch 200: Loss 0.0446
[21:42:50]    Batch 220: Loss 0.0443
[21:43:12]    Batch 240: Loss 0.0441
[21:43:34]    Batch 260: Loss 0.0439
[21:43:56]    Batch 280: Loss 0.0436
[21:44:17]    Batch 300: Loss 0.0434
[21:44:39]    Batch 320: Loss 0.0432
[21:45:00]    Batch 340: Loss 0.0429
[21:45:22]    Batch 360: Loss 0.0427
[21:45:43]    Batch 380: Loss 0.0425
[21:46:04]    Batch 400: Loss 0.0422
[21:46:26]    Batch 420: Loss 0.0420
[21:46:47]    Batch 440: Loss 0.0418
[21:47:09]    Batch 460: Loss 0.0416
[21:47:31]    Batch 480: Loss 0.0413
[21:47:52]    Batch 500: Loss 0.0411
[21:48:13]    Batch 520: Loss 0.0409
[21:48:35]    Batch 540: Loss 0.0407
[21:48:56]    Batch 560: Loss 0.0405
[21:49:18]    Batch 580: Loss 0.0403
[21:49:39]    Batch 600: Loss 0.0401
[21:50:00]    Batch 620: Loss 0.0399
[22:08:23] [Val Epoch 6] raw_min=0.0007 raw_max=0.9999 mode=dist->sim thr_score=0.650 thr_dist=0.362 f1=0.9235 auc=0.9689 acc=0.9235 edge_fb=0
[22:08:32] Epoch 6/50: loss=0.0398, val_loss=0.0424, acc=0.9527, val_acc=0.9235, val_f1=0.9235, val_auc=0.9689, thr=0.362, precision=0.9411, recall=0.9658 [1775.1s/epoch, ETA: 1434.6min]
[22:08:44] [ThresholdSync] Epoch 7: train threshold (dist) = 0.3617
[22:08:44] Starting Epoch 7...
[22:08:44]    Batch 0: Loss 0.0313
[22:08:52]    Batch 20: Loss 0.0325
[22:09:13]    Batch 40: Loss 0.0326
[22:09:35]    Batch 60: Loss 0.0324
[22:09:57]    Batch 80: Loss 0.0323
[22:10:18]    Batch 100: Loss 0.0322
[22:10:40]    Batch 120: Loss 0.0320
[22:11:01]    Batch 140: Loss 0.0319
[22:11:23]    Batch 160: Loss 0.0318
[22:11:44]    Batch 180: Loss 0.0317
[22:12:05]    Batch 200: Loss 0.0315
[22:12:27]    Batch 220: Loss 0.0314
[22:12:49]    Batch 240: Loss 0.0312
[22:13:10]    Batch 260: Loss 0.0311
[22:13:31]    Batch 280: Loss 0.0310
[22:13:53]    Batch 300: Loss 0.0308
[22:14:15]    Batch 320: Loss 0.0307
[22:14:36]    Batch 340: Loss 0.0305
[22:14:58]    Batch 360: Loss 0.0304
[22:15:19]    Batch 380: Loss 0.0303
[22:15:41]    Batch 400: Loss 0.0301
[22:16:02]    Batch 420: Loss 0.0300
[22:16:24]    Batch 440: Loss 0.0299
[22:16:46]    Batch 460: Loss 0.0298
[22:17:07]    Batch 480: Loss 0.0296
[22:17:29]    Batch 500: Loss 0.0295
[22:17:51]    Batch 520: Loss 0.0294
[22:18:12]    Batch 540: Loss 0.0293
[22:18:34]    Batch 560: Loss 0.0291
[22:18:55]    Batch 580: Loss 0.0290
[22:19:17]    Batch 600: Loss 0.0289
[22:19:39]    Batch 620: Loss 0.0288
[22:38:14] [Val Epoch 7] raw_min=0.0002 raw_max=0.9999 mode=dist->sim thr_score=0.610 thr_dist=0.373 f1=0.9361 auc=0.9742 acc=0.9360 edge_fb=0
[22:38:19] Epoch 7/50: loss=0.0288, val_loss=0.0336, acc=0.9542, val_acc=0.9360, val_f1=0.9361, val_auc=0.9742, thr=0.373, precision=0.9425, recall=0.9673 [1787.1s/epoch, ETA: 1385.0min]
[22:38:32] [ThresholdSync] Epoch 8: train threshold (dist) = 0.3731
[22:38:32] Starting Epoch 8...
[22:38:32]    Batch 0: Loss 0.0270
[22:38:41]    Batch 20: Loss 0.0247
[22:39:03]    Batch 40: Loss 0.0246
[22:39:24]    Batch 60: Loss 0.0245
[22:39:46]    Batch 80: Loss 0.0244
[22:40:08]    Batch 100: Loss 0.0243
[22:40:32]    Batch 120: Loss 0.0242
[22:40:58]    Batch 140: Loss 0.0240
[22:41:22]    Batch 160: Loss 0.0240
[22:41:45]    Batch 180: Loss 0.0239
[22:42:07]    Batch 200: Loss 0.0239
[22:42:29]    Batch 220: Loss 0.0238
[22:42:51]    Batch 240: Loss 0.0237
[22:43:13]    Batch 260: Loss 0.0236
[22:43:35]    Batch 280: Loss 0.0235
[22:43:58]    Batch 300: Loss 0.0234
[22:44:21]    Batch 320: Loss 0.0233
[22:44:43]    Batch 340: Loss 0.0232
[22:45:05]    Batch 360: Loss 0.0231
[22:45:27]    Batch 380: Loss 0.0231
[22:45:48]    Batch 400: Loss 0.0230
[22:46:11]    Batch 420: Loss 0.0229
[22:46:33]    Batch 440: Loss 0.0228
[22:46:55]    Batch 460: Loss 0.0228
[22:47:18]    Batch 480: Loss 0.0227
[22:47:42]    Batch 500: Loss 0.0226
[22:48:06]    Batch 520: Loss 0.0225
[22:48:29]    Batch 540: Loss 0.0225
[22:48:52]    Batch 560: Loss 0.0224
[22:49:17]    Batch 580: Loss 0.0223
[22:49:39]    Batch 600: Loss 0.0222
[22:50:02]    Batch 620: Loss 0.0221
[23:09:54] [Val Epoch 8] raw_min=0.0002 raw_max=0.9999 mode=dist->sim thr_score=0.630 thr_dist=0.372 f1=0.9313 auc=0.9696 acc=0.9310 edge_fb=0
[23:09:59] Epoch 8/50: loss=0.0221, val_loss=0.0288, acc=0.9554, val_acc=0.9310, val_f1=0.9313, val_auc=0.9696, thr=0.372, precision=0.9451, recall=0.9670 [1897.3s/epoch, ETA: 1349.9min]
[23:10:10] [ThresholdSync] Epoch 9: train threshold (dist) = 0.3719
[23:10:10] Starting Epoch 9...
[23:10:10]    Batch 0: Loss 0.0202
[23:10:23]    Batch 20: Loss 0.0197
[23:10:46]    Batch 40: Loss 0.0194
[23:11:10]    Batch 60: Loss 0.0194
[23:11:37]    Batch 80: Loss 0.0193
[23:12:00]    Batch 100: Loss 0.0194
[23:12:22]    Batch 120: Loss 0.0193
[23:12:44]    Batch 140: Loss 0.0192
[23:13:06]    Batch 160: Loss 0.0192
[23:13:28]    Batch 180: Loss 0.0191
[23:13:49]    Batch 200: Loss 0.0191
[23:14:11]    Batch 220: Loss 0.0190
[23:14:34]    Batch 240: Loss 0.0190
[23:14:55]    Batch 260: Loss 0.0189
[23:15:17]    Batch 280: Loss 0.0189
[23:15:39]    Batch 300: Loss 0.0188
[23:16:01]    Batch 320: Loss 0.0188
[23:16:23]    Batch 340: Loss 0.0188
[23:16:45]    Batch 360: Loss 0.0187
[23:17:07]    Batch 380: Loss 0.0187
[23:17:28]    Batch 400: Loss 0.0186
[23:17:50]    Batch 420: Loss 0.0186
[23:18:12]    Batch 440: Loss 0.0185
[23:18:34]    Batch 460: Loss 0.0185
[23:18:55]    Batch 480: Loss 0.0185
[23:19:16]    Batch 500: Loss 0.0184
[23:19:38]    Batch 520: Loss 0.0184
[23:19:59]    Batch 540: Loss 0.0183
[23:20:21]    Batch 560: Loss 0.0183
[23:20:44]    Batch 580: Loss 0.0182
[23:21:05]    Batch 600: Loss 0.0182
[23:21:26]    Batch 620: Loss 0.0182
[23:39:46] [Val Epoch 9] raw_min=0.0005 raw_max=0.9999 mode=dist->sim thr_score=0.710 thr_dist=0.339 f1=0.9295 auc=0.9707 acc=0.9295 edge_fb=0
[23:39:53] Epoch 9/50: loss=0.0182, val_loss=0.0263, acc=0.9537, val_acc=0.9295, val_f1=0.9295, val_auc=0.9707, thr=0.339, precision=0.9430, recall=0.9658 [1789.7s/epoch, ETA: 1307.4min]
[23:40:06] [ThresholdSync] Epoch 10: train threshold (dist) = 0.3393
[23:40:06] Starting Epoch 10...
[23:40:06]    Batch 0: Loss 0.0171
[23:40:14]    Batch 20: Loss 0.0170
[23:40:36]    Batch 40: Loss 0.0168
[23:40:57]    Batch 60: Loss 0.0166
[23:41:19]    Batch 80: Loss 0.0165
[23:41:40]    Batch 100: Loss 0.0164
[23:42:02]    Batch 120: Loss 0.0163
[23:42:23]    Batch 140: Loss 0.0163
[23:42:45]    Batch 160: Loss 0.0164
[23:43:06]    Batch 180: Loss 0.0163
[23:43:28]    Batch 200: Loss 0.0163
[23:43:49]    Batch 220: Loss 0.0163
[23:44:11]    Batch 240: Loss 0.0163
[23:44:32]    Batch 260: Loss 0.0163
[23:44:54]    Batch 280: Loss 0.0162
[23:45:15]    Batch 300: Loss 0.0162
[23:45:37]    Batch 320: Loss 0.0161
[23:45:58]    Batch 340: Loss 0.0161
[23:46:20]    Batch 360: Loss 0.0160
[23:46:41]    Batch 380: Loss 0.0160
[23:47:02]    Batch 400: Loss 0.0160
[23:47:24]    Batch 420: Loss 0.0159
[23:47:46]    Batch 440: Loss 0.0159
[23:48:07]    Batch 460: Loss 0.0159
[23:48:29]    Batch 480: Loss 0.0158
[23:48:50]    Batch 500: Loss 0.0158
[23:49:12]    Batch 520: Loss 0.0158
[23:49:33]    Batch 540: Loss 0.0157
[23:49:55]    Batch 560: Loss 0.0157
[23:50:16]    Batch 580: Loss 0.0157
[23:50:37]    Batch 600: Loss 0.0157
[23:50:59]    Batch 620: Loss 0.0156
[00:09:19] [Val Epoch 10] raw_min=0.0001 raw_max=0.9999 mode=dist->sim thr_score=0.640 thr_dist=0.348 f1=0.9411 auc=0.9759 acc=0.9410 edge_fb=0
[00:09:27] Epoch 10/50: loss=0.0156, val_loss=0.0237, acc=0.9569, val_acc=0.9410, val_f1=0.9411, val_auc=0.9759, thr=0.348, precision=0.9468, recall=0.9682 [1770.8s/epoch, ETA: 1266.2min]
[00:09:40] [ThresholdSync] Epoch 11: train threshold (dist) = 0.3476
[00:09:40] Starting Epoch 11...
[00:09:40]    Batch 0: Loss 0.0137
[00:09:48]    Batch 20: Loss 0.0145
[00:10:09]    Batch 40: Loss 0.0145
[00:10:31]    Batch 60: Loss 0.0144
[00:10:52]    Batch 80: Loss 0.0145
[00:11:13]    Batch 100: Loss 0.0146
[00:11:35]    Batch 120: Loss 0.0145
[00:11:56]    Batch 140: Loss 0.0145
[00:12:18]    Batch 160: Loss 0.0144
[00:12:40]    Batch 180: Loss 0.0144
[00:13:00]    Batch 200: Loss 0.0145
[00:13:22]    Batch 220: Loss 0.0144
[00:13:43]    Batch 240: Loss 0.0144
[00:14:05]    Batch 260: Loss 0.0144
[00:14:26]    Batch 280: Loss 0.0144
[00:14:48]    Batch 300: Loss 0.0144
[00:15:09]    Batch 320: Loss 0.0144
[00:15:31]    Batch 340: Loss 0.0144
[00:15:53]    Batch 360: Loss 0.0143
[00:16:14]    Batch 380: Loss 0.0143
[00:16:36]    Batch 400: Loss 0.0143
[00:16:57]    Batch 420: Loss 0.0143
[00:17:19]    Batch 440: Loss 0.0143
[00:17:40]    Batch 460: Loss 0.0143
[00:18:02]    Batch 480: Loss 0.0142
[00:18:23]    Batch 500: Loss 0.0142
[00:18:45]    Batch 520: Loss 0.0142
[00:19:06]    Batch 540: Loss 0.0142
[00:19:27]    Batch 560: Loss 0.0141
[00:19:49]    Batch 580: Loss 0.0141
[00:20:10]    Batch 600: Loss 0.0141
[00:20:31]    Batch 620: Loss 0.0141
[00:38:52] [Val Epoch 11] raw_min=0.0001 raw_max=0.9999 mode=dist->sim thr_score=0.640 thr_dist=0.353 f1=0.9427 auc=0.9733 acc=0.9425 edge_fb=0
[00:38:57] Epoch 11/50: loss=0.0141, val_loss=0.0221, acc=0.9578, val_acc=0.9425, val_f1=0.9427, val_auc=0.9733, thr=0.353, precision=0.9489, recall=0.9676 [1770.3s/epoch, ETA: 1227.1min]
[00:39:05] Training completed in 346.2 minutes
[00:39:05] Loaded best checkpoint before final save: best_model_epoch_04_auc_0.9795_vloss_0.0828_vf1_0.9405.weights.h5 (val_auc=0.9795)
[00:39:18] Final model saved to: models\Proposed Siamese CapsNet + MobileNetV2_20251007_185241\models\final_best_model, models\Proposed Siamese CapsNet + MobileNetV2_20251007_185241\models\final_best_model.keras and weights models\Proposed Siamese CapsNet + MobileNetV2_20251007_185241\models\final_best_model.weights.h5
[00:39:18] Training completed successfully.

