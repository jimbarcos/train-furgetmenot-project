[03:30:58] Data generators loaded from CSV files
[03:30:58] Welcome to Real-Time Model Training Interface!
[03:30:58] Select a model and configure parameters to begin training.
[03:31:09] Loading proposed model...
[03:31:12] Proposed Siamese CapsNet + MobileNetV2 loaded successfully.
[03:31:18] Reloading data generators with current batch size...
[03:31:26] Data generators loaded from CSV files
[03:31:26] Model directory created: models\Proposed Siamese CapsNet + MobileNetV2_20251006_033126
[03:31:26] Starting training for 50 epochs...
[03:31:26] Preparing model and data for training...
[03:31:28] [RoutingEntropy] mean_norm_entropy=0.9933
[03:31:29] Adaptive threshold mechanism enabled
[03:31:29] Batch sizes - Train: 32, Val: 16
[03:31:29] Data ready - Train: 625 batches, Val: 625 batches
[03:31:29] Training with learning rate: 5.00e-05
[03:31:29] Mixed precision training enabled
[03:31:29] Starting training with 50 epochs...
[03:31:29] Model parameters: 3,242,585
[03:31:29] [Pre-Train Distances] n=16 min=0.03415 max=0.10730 mean=0.05726 std=0.01982 spread=0.07315
[03:31:29] Models will be saved to: models\Proposed Siamese CapsNet + MobileNetV2_20251006_033126\models
[03:31:29] Training started - Preparing data...
[03:31:29] [ThresholdSync] Epoch 1: train threshold (dist) = 0.5000
[03:31:29] Starting Epoch 1...
[03:31:45]    Batch 0: Loss 1.0882
[03:32:02]    Batch 20: Loss 1.0702
[03:32:19]    Batch 40: Loss 1.0532
[03:32:36]    Batch 60: Loss 1.0371
[03:32:52]    Batch 80: Loss 1.0227
[03:33:08]    Batch 100: Loss 1.0091
[03:33:24]    Batch 120: Loss 0.9964
[03:33:41]    Batch 140: Loss 0.9841
[03:33:57]    Batch 160: Loss 0.9722
[03:34:14]    Batch 180: Loss 0.9607
[03:34:30]    Batch 200: Loss 0.9494
[03:34:46]    Batch 220: Loss 0.9385
[03:35:02]    Batch 240: Loss 0.9278
[03:35:19]    Batch 260: Loss 0.9174
[03:35:35]    Batch 280: Loss 0.9071
[03:35:51]    Batch 300: Loss 0.8971
[03:36:07]    Batch 320: Loss 0.8871
[03:36:24]    Batch 340: Loss 0.8774
[03:36:41]    Batch 360: Loss 0.8679
[03:36:57]    Batch 380: Loss 0.8585
[03:37:13]    Batch 400: Loss 0.8492
[03:37:30]    Batch 420: Loss 0.8401
[03:37:47]    Batch 440: Loss 0.8312
[03:38:03]    Batch 460: Loss 0.8224
[03:38:19]    Batch 480: Loss 0.8138
[03:38:35]    Batch 500: Loss 0.8052
[03:38:51]    Batch 520: Loss 0.7969
[03:39:07]    Batch 540: Loss 0.7886
[03:39:23]    Batch 560: Loss 0.7804
[03:39:40]    Batch 580: Loss 0.7724
[03:39:56]    Batch 600: Loss 0.7645
[03:40:12]    Batch 620: Loss 0.7568
[03:52:55] [Val Epoch 1] raw_min=0.0021 raw_max=0.9999 mode=dist->sim thr_score=0.650 thr_dist=0.441 f1=0.9011 auc=0.9563 acc=0.9010 edge_fb=0
[03:52:58] Epoch 1/50: loss=0.7552, val_loss=0.5236, acc=0.7706, val_acc=0.9010, val_f1=0.9011, val_auc=0.9563, thr=0.441, precision=0.7904, recall=0.7364 [1285.4s/epoch, ETA: 1049.8min]
[03:53:06] [ThresholdSync] Epoch 2: train threshold (dist) = 0.4405
[03:53:06] Starting Epoch 2...
[03:53:06]    Batch 0: Loss 0.5170
[03:53:18]    Batch 20: Loss 0.5091
[03:53:34]    Batch 40: Loss 0.5028
[03:53:50]    Batch 60: Loss 0.4969
[03:54:07]    Batch 80: Loss 0.4914
[03:54:23]    Batch 100: Loss 0.4858
[03:54:39]    Batch 120: Loss 0.4804
[03:54:55]    Batch 140: Loss 0.4753
[03:55:11]    Batch 160: Loss 0.4701
[03:55:27]    Batch 180: Loss 0.4649
[03:55:43]    Batch 200: Loss 0.4599
[03:56:00]    Batch 220: Loss 0.4549
[03:56:16]    Batch 240: Loss 0.4501
[03:56:32]    Batch 260: Loss 0.4452
[03:56:48]    Batch 280: Loss 0.4405
[03:57:04]    Batch 300: Loss 0.4358
[03:57:20]    Batch 320: Loss 0.4311
[03:57:37]    Batch 340: Loss 0.4266
[03:57:53]    Batch 360: Loss 0.4221
[03:58:09]    Batch 380: Loss 0.4178
[03:58:25]    Batch 400: Loss 0.4134
[03:58:41]    Batch 420: Loss 0.4091
[03:58:57]    Batch 440: Loss 0.4049
[03:59:14]    Batch 460: Loss 0.4007
[03:59:30]    Batch 480: Loss 0.3967
[03:59:46]    Batch 500: Loss 0.3927
[04:00:02]    Batch 520: Loss 0.3887
[04:00:18]    Batch 540: Loss 0.3848
[04:00:34]    Batch 560: Loss 0.3810
[04:00:51]    Batch 580: Loss 0.3772
[04:01:07]    Batch 600: Loss 0.3734
[04:01:23]    Batch 620: Loss 0.3697
[04:14:06] [Val Epoch 2] raw_min=0.0014 raw_max=0.9999 mode=dist->sim thr_score=0.630 thr_dist=0.413 f1=0.9236 auc=0.9639 acc=0.9235 edge_fb=0
[04:14:10] Epoch 2/50: loss=0.3690, val_loss=0.2630, acc=0.8924, val_acc=0.9235, val_f1=0.9236, val_auc=0.9639, thr=0.413, precision=0.9072, recall=0.8741 [1266.2s/epoch, ETA: 1022.6min]
[04:14:20] [ThresholdSync] Epoch 3: train threshold (dist) = 0.4127
[04:14:20] Starting Epoch 3...
[04:14:20]    Batch 0: Loss 0.2530
[04:14:29]    Batch 20: Loss 0.2511
[04:14:45]    Batch 40: Loss 0.2492
[04:15:01]    Batch 60: Loss 0.2465
[04:15:17]    Batch 80: Loss 0.2439
[04:15:33]    Batch 100: Loss 0.2413
[04:15:49]    Batch 120: Loss 0.2389
[04:16:05]    Batch 140: Loss 0.2363
[04:16:22]    Batch 160: Loss 0.2338
[04:16:38]    Batch 180: Loss 0.2314
[04:16:54]    Batch 200: Loss 0.2291
[04:17:10]    Batch 220: Loss 0.2267
[04:17:26]    Batch 240: Loss 0.2245
[04:17:42]    Batch 260: Loss 0.2222
[04:17:58]    Batch 280: Loss 0.2199
[04:18:14]    Batch 300: Loss 0.2177
[04:18:30]    Batch 320: Loss 0.2156
[04:18:47]    Batch 340: Loss 0.2134
[04:19:03]    Batch 360: Loss 0.2113
[04:19:19]    Batch 380: Loss 0.2092
[04:19:35]    Batch 400: Loss 0.2072
[04:19:51]    Batch 420: Loss 0.2053
[04:20:07]    Batch 440: Loss 0.2033
[04:20:23]    Batch 460: Loss 0.2014
[04:20:40]    Batch 480: Loss 0.1994
[04:20:55]    Batch 500: Loss 0.1975
[04:21:12]    Batch 520: Loss 0.1957
[04:21:28]    Batch 540: Loss 0.1938
[04:21:44]    Batch 560: Loss 0.1920
[04:22:00]    Batch 580: Loss 0.1902
[04:22:16]    Batch 600: Loss 0.1885
[04:22:32]    Batch 620: Loss 0.1868
[04:35:12] [Val Epoch 3] raw_min=0.0012 raw_max=0.9999 mode=dist->sim thr_score=0.600 thr_dist=0.408 f1=0.9387 auc=0.9722 acc=0.9385 edge_fb=0
[04:35:16] Epoch 3/50: loss=0.1864, val_loss=0.1412, acc=0.9227, val_acc=0.9385, val_f1=0.9387, val_auc=0.9722, thr=0.408, precision=0.9277, recall=0.9168 [1262.1s/epoch, ETA: 998.3min]
[04:35:26] [ThresholdSync] Epoch 4: train threshold (dist) = 0.4079
[04:35:26] Starting Epoch 4...
[04:35:26]    Batch 0: Loss 0.1321
[04:35:36]    Batch 20: Loss 0.1312
[04:35:52]    Batch 40: Loss 0.1299
[04:36:08]    Batch 60: Loss 0.1286
[04:36:24]    Batch 80: Loss 0.1274
[04:36:40]    Batch 100: Loss 0.1263
[04:36:57]    Batch 120: Loss 0.1252
[04:37:13]    Batch 140: Loss 0.1240
[04:37:29]    Batch 160: Loss 0.1229
[04:37:45]    Batch 180: Loss 0.1218
[04:38:01]    Batch 200: Loss 0.1207
[04:38:17]    Batch 220: Loss 0.1196
[04:38:34]    Batch 240: Loss 0.1186
[04:38:50]    Batch 260: Loss 0.1176
[04:39:06]    Batch 280: Loss 0.1165
[04:39:22]    Batch 300: Loss 0.1155
[04:39:38]    Batch 320: Loss 0.1145
[04:39:54]    Batch 340: Loss 0.1136
[04:40:11]    Batch 360: Loss 0.1126
[04:40:27]    Batch 380: Loss 0.1116
[04:40:43]    Batch 400: Loss 0.1107
[04:40:59]    Batch 420: Loss 0.1098
[04:41:15]    Batch 440: Loss 0.1088
[04:41:31]    Batch 460: Loss 0.1080
[04:41:48]    Batch 480: Loss 0.1070
[04:42:04]    Batch 500: Loss 0.1061
[04:42:20]    Batch 520: Loss 0.1053
[04:42:36]    Batch 540: Loss 0.1044
[04:42:52]    Batch 560: Loss 0.1036
[04:43:08]    Batch 580: Loss 0.1027
[04:43:24]    Batch 600: Loss 0.1019
[04:43:40]    Batch 620: Loss 0.1011
[04:56:22] [Val Epoch 4] raw_min=0.0010 raw_max=0.9999 mode=dist->sim thr_score=0.610 thr_dist=0.401 f1=0.9355 auc=0.9764 acc=0.9355 edge_fb=0
[04:56:27] Epoch 4/50: loss=0.1010, val_loss=0.0842, acc=0.9419, val_acc=0.9355, val_f1=0.9355, val_auc=0.9764, thr=0.401, precision=0.9383, recall=0.9459 [1264.6s/epoch, ETA: 976.1min]
[04:56:37] [ThresholdSync] Epoch 5: train threshold (dist) = 0.4010
[04:56:37] Starting Epoch 5...
[04:56:37]    Batch 0: Loss 0.0777
[04:56:46]    Batch 20: Loss 0.0754
[04:57:02]    Batch 40: Loss 0.0748
[04:57:18]    Batch 60: Loss 0.0740
[04:57:34]    Batch 80: Loss 0.0734
[04:57:51]    Batch 100: Loss 0.0728
[04:58:07]    Batch 120: Loss 0.0723
[04:58:24]    Batch 140: Loss 0.0718
[04:58:40]    Batch 160: Loss 0.0712
[04:58:57]    Batch 180: Loss 0.0707
[04:59:13]    Batch 200: Loss 0.0702
[04:59:29]    Batch 220: Loss 0.0697
[04:59:45]    Batch 240: Loss 0.0691
[05:00:01]    Batch 260: Loss 0.0686
[05:00:18]    Batch 280: Loss 0.0681
[05:00:34]    Batch 300: Loss 0.0676
[05:00:50]    Batch 320: Loss 0.0671
[05:01:06]    Batch 340: Loss 0.0666
[05:01:22]    Batch 360: Loss 0.0662
[05:01:38]    Batch 380: Loss 0.0657
[05:01:54]    Batch 400: Loss 0.0652
[05:02:11]    Batch 420: Loss 0.0648
[05:02:27]    Batch 440: Loss 0.0643
[05:02:43]    Batch 460: Loss 0.0639
[05:02:59]    Batch 480: Loss 0.0635
[05:03:15]    Batch 500: Loss 0.0630
[05:03:31]    Batch 520: Loss 0.0626
[05:03:47]    Batch 540: Loss 0.0622
[05:04:03]    Batch 560: Loss 0.0618
[05:04:19]    Batch 580: Loss 0.0614
[05:04:35]    Batch 600: Loss 0.0610
[05:04:52]    Batch 620: Loss 0.0606
[05:17:32] [Val Epoch 5] raw_min=0.0006 raw_max=0.9999 mode=dist->sim thr_score=0.630 thr_dist=0.389 f1=0.9393 auc=0.9747 acc=0.9395 edge_fb=0
[05:17:40] Epoch 5/50: loss=0.0605, val_loss=0.0567, acc=0.9478, val_acc=0.9395, val_f1=0.9393, val_auc=0.9747, thr=0.389, precision=0.9435, recall=0.9526 [1265.0s/epoch, ETA: 954.5min]
[05:17:51] [ThresholdSync] Epoch 6: train threshold (dist) = 0.3887
[05:17:51] Starting Epoch 6...
[05:17:51]    Batch 0: Loss 0.0492
[05:17:56]    Batch 20: Loss 0.0473
[05:18:12]    Batch 40: Loss 0.0470
[05:18:28]    Batch 60: Loss 0.0469
[05:18:44]    Batch 80: Loss 0.0467
[05:19:00]    Batch 100: Loss 0.0463
[05:19:16]    Batch 120: Loss 0.0459
[05:19:32]    Batch 140: Loss 0.0457
[05:19:49]    Batch 160: Loss 0.0454
[05:20:05]    Batch 180: Loss 0.0451
[05:20:21]    Batch 200: Loss 0.0449
[05:20:37]    Batch 220: Loss 0.0446
[05:20:53]    Batch 240: Loss 0.0443
[05:21:09]    Batch 260: Loss 0.0441
[05:21:26]    Batch 280: Loss 0.0438
[05:21:42]    Batch 300: Loss 0.0436
[05:21:58]    Batch 320: Loss 0.0433
[05:22:14]    Batch 340: Loss 0.0431
[05:22:30]    Batch 360: Loss 0.0429
[05:22:46]    Batch 380: Loss 0.0426
[05:23:02]    Batch 400: Loss 0.0424
[05:23:18]    Batch 420: Loss 0.0421
[05:23:35]    Batch 440: Loss 0.0419
[05:23:51]    Batch 460: Loss 0.0417
[05:24:07]    Batch 480: Loss 0.0414
[05:24:23]    Batch 500: Loss 0.0412
[05:24:39]    Batch 520: Loss 0.0410
[05:24:55]    Batch 540: Loss 0.0408
[05:25:12]    Batch 560: Loss 0.0405
[05:25:28]    Batch 580: Loss 0.0403
[05:25:44]    Batch 600: Loss 0.0401
[05:26:00]    Batch 620: Loss 0.0399
[05:38:43] [Val Epoch 6] raw_min=0.0005 raw_max=0.9999 mode=dist->sim thr_score=0.650 thr_dist=0.373 f1=0.9276 auc=0.9729 acc=0.9275 edge_fb=0
[05:38:50] Epoch 6/50: loss=0.0399, val_loss=0.0425, acc=0.9500, val_acc=0.9275, val_f1=0.9276, val_auc=0.9729, thr=0.373, precision=0.9415, recall=0.9596 [1267.9s/epoch, ETA: 933.0min]
[05:39:00] [ThresholdSync] Epoch 7: train threshold (dist) = 0.3733
[05:39:00] Starting Epoch 7...
[05:39:00]    Batch 0: Loss 0.0338
[05:39:05]    Batch 20: Loss 0.0328
[05:39:22]    Batch 40: Loss 0.0329
[05:39:38]    Batch 60: Loss 0.0325
[05:39:54]    Batch 80: Loss 0.0323
[05:40:10]    Batch 100: Loss 0.0321
[05:40:26]    Batch 120: Loss 0.0320
[05:40:43]    Batch 140: Loss 0.0318
[05:40:59]    Batch 160: Loss 0.0317
[05:41:15]    Batch 180: Loss 0.0315
[05:41:31]    Batch 200: Loss 0.0314
[05:41:47]    Batch 220: Loss 0.0312
[05:42:03]    Batch 240: Loss 0.0311
[05:42:20]    Batch 260: Loss 0.0309
[05:42:36]    Batch 280: Loss 0.0307
[05:42:52]    Batch 300: Loss 0.0306
[05:43:08]    Batch 320: Loss 0.0304
[05:43:24]    Batch 340: Loss 0.0303
[05:43:41]    Batch 360: Loss 0.0302
[05:43:57]    Batch 380: Loss 0.0300
[05:44:13]    Batch 400: Loss 0.0298
[05:44:29]    Batch 420: Loss 0.0297
[05:44:45]    Batch 440: Loss 0.0296
[05:45:01]    Batch 460: Loss 0.0295
[05:45:18]    Batch 480: Loss 0.0293
[05:45:34]    Batch 500: Loss 0.0292
[05:45:50]    Batch 520: Loss 0.0291
[05:46:06]    Batch 540: Loss 0.0289
[05:46:22]    Batch 560: Loss 0.0288
[05:46:38]    Batch 580: Loss 0.0287
[05:46:54]    Batch 600: Loss 0.0286
[05:47:11]    Batch 620: Loss 0.0284
[05:59:52] [Val Epoch 7] raw_min=0.0006 raw_max=0.9999 mode=dist->sim thr_score=0.660 thr_dist=0.360 f1=0.9382 auc=0.9804 acc=0.9380 edge_fb=0
[05:59:56] Epoch 7/50: loss=0.0284, val_loss=0.0333, acc=0.9523, val_acc=0.9380, val_f1=0.9382, val_auc=0.9804, thr=0.360, precision=0.9429, recall=0.9629 [1267.2s/epoch, ETA: 911.5min]
[06:00:06] [ThresholdSync] Epoch 8: train threshold (dist) = 0.3601
[06:00:06] Starting Epoch 8...
[06:00:06]    Batch 0: Loss 0.0230
[06:00:16]    Batch 20: Loss 0.0238
[06:00:32]    Batch 40: Loss 0.0239
[06:00:48]    Batch 60: Loss 0.0239
[06:01:04]    Batch 80: Loss 0.0237
[06:01:20]    Batch 100: Loss 0.0236
[06:01:37]    Batch 120: Loss 0.0235
[06:01:53]    Batch 140: Loss 0.0234
[06:02:09]    Batch 160: Loss 0.0233
[06:02:25]    Batch 180: Loss 0.0232
[06:02:41]    Batch 200: Loss 0.0231
[06:02:57]    Batch 220: Loss 0.0230
[06:03:14]    Batch 240: Loss 0.0229
[06:03:30]    Batch 260: Loss 0.0229
[06:03:46]    Batch 280: Loss 0.0228
[06:04:02]    Batch 300: Loss 0.0227
[06:04:18]    Batch 320: Loss 0.0226
[06:04:34]    Batch 340: Loss 0.0226
[06:04:50]    Batch 360: Loss 0.0225
[06:05:06]    Batch 380: Loss 0.0224
[06:05:22]    Batch 400: Loss 0.0223
[06:05:39]    Batch 420: Loss 0.0223
[06:05:55]    Batch 440: Loss 0.0222
[06:06:11]    Batch 460: Loss 0.0221
[06:06:27]    Batch 480: Loss 0.0220
[06:06:43]    Batch 500: Loss 0.0219
[06:07:00]    Batch 520: Loss 0.0219
[06:07:16]    Batch 540: Loss 0.0218
[06:07:32]    Batch 560: Loss 0.0217
[06:07:48]    Batch 580: Loss 0.0216
[06:08:04]    Batch 600: Loss 0.0216
[06:08:20]    Batch 620: Loss 0.0215
[06:21:02] [Val Epoch 8] raw_min=0.0005 raw_max=0.9999 mode=dist->sim thr_score=0.650 thr_dist=0.356 f1=0.9240 auc=0.9705 acc=0.9240 edge_fb=0
[06:21:09] Epoch 8/50: loss=0.0215, val_loss=0.0285, acc=0.9582, val_acc=0.9240, val_f1=0.9240, val_auc=0.9705, thr=0.356, precision=0.9493, recall=0.9680 [1265.0s/epoch, ETA: 890.1min]
[06:21:22] [ThresholdSync] Epoch 9: train threshold (dist) = 0.3562
[06:21:22] Starting Epoch 9...
[06:21:22]    Batch 0: Loss 0.0182
[06:21:25]    Batch 20: Loss 0.0191
[06:21:41]    Batch 40: Loss 0.0189
[06:21:58]    Batch 60: Loss 0.0188
[06:22:14]    Batch 80: Loss 0.0187
[06:22:30]    Batch 100: Loss 0.0186
[06:22:46]    Batch 120: Loss 0.0186
[06:23:02]    Batch 140: Loss 0.0186
[06:23:19]    Batch 160: Loss 0.0185
[06:23:35]    Batch 180: Loss 0.0184
[06:23:51]    Batch 200: Loss 0.0184
[06:24:07]    Batch 220: Loss 0.0183
[06:24:23]    Batch 240: Loss 0.0182
[06:24:39]    Batch 260: Loss 0.0181
[06:24:55]    Batch 280: Loss 0.0180
[06:25:11]    Batch 300: Loss 0.0180
[06:25:28]    Batch 320: Loss 0.0180
[06:25:44]    Batch 340: Loss 0.0179
[06:26:00]    Batch 360: Loss 0.0179
[06:26:17]    Batch 380: Loss 0.0179
[06:26:32]    Batch 400: Loss 0.0178
[06:26:49]    Batch 420: Loss 0.0178
[06:27:05]    Batch 440: Loss 0.0178
[06:27:21]    Batch 460: Loss 0.0177
[06:27:37]    Batch 480: Loss 0.0177
[06:27:53]    Batch 500: Loss 0.0176
[06:28:09]    Batch 520: Loss 0.0176
[06:28:27]    Batch 540: Loss 0.0175
[06:28:44]    Batch 560: Loss 0.0175
[06:29:01]    Batch 580: Loss 0.0174
[06:29:17]    Batch 600: Loss 0.0174
[06:29:33]    Batch 620: Loss 0.0173
[06:42:24] [Val Epoch 9] raw_min=0.0004 raw_max=0.9999 mode=dist->sim thr_score=0.690 thr_dist=0.338 f1=0.9211 auc=0.9686 acc=0.9210 edge_fb=0
[06:42:30] Epoch 9/50: loss=0.0173, val_loss=0.0252, acc=0.9582, val_acc=0.9210, val_f1=0.9211, val_auc=0.9686, thr=0.338, precision=0.9485, recall=0.9691 [1279.5s/epoch, ETA: 869.7min]
[06:42:41] [ThresholdSync] Epoch 10: train threshold (dist) = 0.3378
[06:42:41] Starting Epoch 10...
[06:42:41]    Batch 0: Loss 0.0151
[06:42:48]    Batch 20: Loss 0.0156
[06:43:05]    Batch 40: Loss 0.0156
[06:43:21]    Batch 60: Loss 0.0155
[06:43:38]    Batch 80: Loss 0.0154
[06:43:55]    Batch 100: Loss 0.0154
[06:44:11]    Batch 120: Loss 0.0154
[06:44:28]    Batch 140: Loss 0.0153
[06:44:44]    Batch 160: Loss 0.0153
[06:45:01]    Batch 180: Loss 0.0154
[06:45:17]    Batch 200: Loss 0.0153
[06:45:33]    Batch 220: Loss 0.0153
[06:45:50]    Batch 240: Loss 0.0153
[06:46:06]    Batch 260: Loss 0.0152
[06:46:23]    Batch 280: Loss 0.0152
[06:46:39]    Batch 300: Loss 0.0151
[06:46:56]    Batch 320: Loss 0.0151
[06:47:12]    Batch 340: Loss 0.0151
[06:47:29]    Batch 360: Loss 0.0151
[06:47:45]    Batch 380: Loss 0.0151
[06:48:01]    Batch 400: Loss 0.0150
[06:48:17]    Batch 420: Loss 0.0150
[06:48:33]    Batch 440: Loss 0.0150
[06:48:50]    Batch 460: Loss 0.0150
[06:49:06]    Batch 480: Loss 0.0149
[06:49:23]    Batch 500: Loss 0.0149
[06:49:39]    Batch 520: Loss 0.0149
[06:49:55]    Batch 540: Loss 0.0148
[06:50:12]    Batch 560: Loss 0.0148
[06:50:28]    Batch 580: Loss 0.0148
[06:50:44]    Batch 600: Loss 0.0147
[06:51:01]    Batch 620: Loss 0.0147
[07:03:44] [Val Epoch 10] raw_min=0.0005 raw_max=0.9999 mode=dist->sim thr_score=0.610 thr_dist=0.359 f1=0.9259 auc=0.9688 acc=0.9255 edge_fb=0
[07:03:51] Epoch 10/50: loss=0.0147, val_loss=0.0232, acc=0.9574, val_acc=0.9255, val_f1=0.9259, val_auc=0.9688, thr=0.359, precision=0.9531, recall=0.9622 [1277.3s/epoch, ETA: 849.0min]
[07:04:02] [ThresholdSync] Epoch 11: train threshold (dist) = 0.3588
[07:04:02] Starting Epoch 11...
[07:04:02]    Batch 0: Loss 0.0150
[07:04:07]    Batch 20: Loss 0.0138
[07:04:24]    Batch 40: Loss 0.0136
[07:04:40]    Batch 60: Loss 0.0136
[07:04:56]    Batch 80: Loss 0.0135
[07:05:12]    Batch 100: Loss 0.0134
[07:05:28]    Batch 120: Loss 0.0135
[07:05:44]    Batch 140: Loss 0.0135
[07:06:01]    Batch 160: Loss 0.0135
[07:06:17]    Batch 180: Loss 0.0134
[07:06:33]    Batch 200: Loss 0.0134
[07:06:49]    Batch 220: Loss 0.0134
[07:07:05]    Batch 240: Loss 0.0134
[07:07:22]    Batch 260: Loss 0.0134
[07:07:38]    Batch 280: Loss 0.0134
[07:07:54]    Batch 300: Loss 0.0133
[07:08:10]    Batch 320: Loss 0.0133
[07:08:27]    Batch 340: Loss 0.0133
[07:08:43]    Batch 360: Loss 0.0133
[07:08:59]    Batch 380: Loss 0.0133
[07:09:15]    Batch 400: Loss 0.0132
[07:09:31]    Batch 420: Loss 0.0132
[07:09:48]    Batch 440: Loss 0.0132
[07:10:04]    Batch 460: Loss 0.0132
[07:10:20]    Batch 480: Loss 0.0132
[07:10:36]    Batch 500: Loss 0.0131
[07:10:53]    Batch 520: Loss 0.0131
[07:11:09]    Batch 540: Loss 0.0131
[07:11:25]    Batch 560: Loss 0.0131
[07:11:42]    Batch 580: Loss 0.0131
[07:11:58]    Batch 600: Loss 0.0131
[07:12:14]    Batch 620: Loss 0.0131
[07:24:57] [Val Epoch 11] raw_min=0.0002 raw_max=0.9999 mode=dist->sim thr_score=0.680 thr_dist=0.343 f1=0.9265 auc=0.9674 acc=0.9265 edge_fb=0
[07:25:04] Epoch 11/50: loss=0.0131, val_loss=0.0220, acc=0.9620, val_acc=0.9265, val_f1=0.9265, val_auc=0.9674, thr=0.343, precision=0.9572, recall=0.9673 [1270.6s/epoch, ETA: 827.7min]
[07:25:15] [ThresholdSync] Epoch 12: train threshold (dist) = 0.3433
[07:25:15] Starting Epoch 12...
[07:25:15]    Batch 0: Loss 0.0110
[07:25:20]    Batch 20: Loss 0.0117
[07:25:36]    Batch 40: Loss 0.0120
[07:25:52]    Batch 60: Loss 0.0118
[07:26:08]    Batch 80: Loss 0.0120
[07:26:25]    Batch 100: Loss 0.0120
[07:26:41]    Batch 120: Loss 0.0120
[07:26:57]    Batch 140: Loss 0.0121
[07:27:13]    Batch 160: Loss 0.0121
[07:27:29]    Batch 180: Loss 0.0121
[07:27:45]    Batch 200: Loss 0.0121
[07:28:02]    Batch 220: Loss 0.0121
[07:28:18]    Batch 240: Loss 0.0121
[07:28:36]    Batch 260: Loss 0.0121
[07:28:53]    Batch 280: Loss 0.0121
[07:29:09]    Batch 300: Loss 0.0121
[07:29:25]    Batch 320: Loss 0.0121
[07:29:42]    Batch 340: Loss 0.0120
[07:29:58]    Batch 360: Loss 0.0120
[07:30:14]    Batch 380: Loss 0.0120
[07:30:30]    Batch 400: Loss 0.0120
[07:30:47]    Batch 420: Loss 0.0120
[07:31:03]    Batch 440: Loss 0.0120
[07:31:19]    Batch 460: Loss 0.0120
[07:31:35]    Batch 480: Loss 0.0120
[07:31:51]    Batch 500: Loss 0.0120
[07:32:07]    Batch 520: Loss 0.0120
[07:32:24]    Batch 540: Loss 0.0120
[07:32:40]    Batch 560: Loss 0.0120
[07:32:56]    Batch 580: Loss 0.0120
[07:33:12]    Batch 600: Loss 0.0120
[07:33:28]    Batch 620: Loss 0.0120
[07:46:11] [Val Epoch 12] raw_min=0.0003 raw_max=0.9999 mode=dist->sim thr_score=0.600 thr_dist=0.366 f1=0.9265 auc=0.9712 acc=0.9265 edge_fb=0
[07:46:19] Epoch 12/50: loss=0.0120, val_loss=0.0216, acc=0.9629, val_acc=0.9265, val_f1=0.9265, val_auc=0.9712, thr=0.366, precision=0.9591, recall=0.9670 [1272.1s/epoch, ETA: 806.6min]
[07:46:30] [ThresholdSync] Epoch 13: train threshold (dist) = 0.3660
[07:46:30] Starting Epoch 13...
[07:46:30]    Batch 0: Loss 0.0109
[07:46:35]    Batch 20: Loss 0.0112
[07:46:52]    Batch 40: Loss 0.0114
[07:47:08]    Batch 60: Loss 0.0114
[07:47:24]    Batch 80: Loss 0.0113
[07:47:40]    Batch 100: Loss 0.0114
[07:47:56]    Batch 120: Loss 0.0113
[07:48:12]    Batch 140: Loss 0.0114
[07:48:28]    Batch 160: Loss 0.0113
[07:48:45]    Batch 180: Loss 0.0113
[07:49:01]    Batch 200: Loss 0.0113
[07:49:17]    Batch 220: Loss 0.0114
[07:49:33]    Batch 240: Loss 0.0113
[07:49:50]    Batch 260: Loss 0.0113
[07:50:06]    Batch 280: Loss 0.0113
[07:50:22]    Batch 300: Loss 0.0113
[07:50:38]    Batch 320: Loss 0.0113
[07:50:54]    Batch 340: Loss 0.0112
[07:51:10]    Batch 360: Loss 0.0112
[07:51:27]    Batch 380: Loss 0.0112
[07:51:43]    Batch 400: Loss 0.0112
[07:51:59]    Batch 420: Loss 0.0112
[07:52:16]    Batch 440: Loss 0.0112
[07:52:32]    Batch 460: Loss 0.0112
[07:52:48]    Batch 480: Loss 0.0112
[07:53:04]    Batch 500: Loss 0.0112
[07:53:20]    Batch 520: Loss 0.0112
[07:53:36]    Batch 540: Loss 0.0112
[07:53:53]    Batch 560: Loss 0.0112
[07:54:09]    Batch 580: Loss 0.0112
[07:54:25]    Batch 600: Loss 0.0112
[07:54:41]    Batch 620: Loss 0.0112
[08:07:24] [Val Epoch 13] raw_min=0.0004 raw_max=0.9999 mode=dist->sim thr_score=0.640 thr_dist=0.364 f1=0.9266 auc=0.9658 acc=0.9265 edge_fb=0
[08:07:29] Epoch 13/50: loss=0.0112, val_loss=0.0209, acc=0.9651, val_acc=0.9265, val_f1=0.9266, val_auc=0.9658, thr=0.364, precision=0.9631, recall=0.9674 [1269.9s/epoch, ETA: 785.3min]
[08:07:40] [ThresholdSync] Epoch 14: train threshold (dist) = 0.3637
[08:07:40] Starting Epoch 14...
[08:07:40]    Batch 0: Loss 0.0108
[08:07:46]    Batch 20: Loss 0.0106
[08:08:03]    Batch 40: Loss 0.0106
[08:08:19]    Batch 60: Loss 0.0106
[08:08:35]    Batch 80: Loss 0.0106
[08:08:51]    Batch 100: Loss 0.0105
[08:09:07]    Batch 120: Loss 0.0106
[08:09:24]    Batch 140: Loss 0.0106
[08:09:40]    Batch 160: Loss 0.0106
[08:09:56]    Batch 180: Loss 0.0107
[08:10:12]    Batch 200: Loss 0.0107
[08:10:28]    Batch 220: Loss 0.0106
[08:10:45]    Batch 240: Loss 0.0106
[08:11:01]    Batch 260: Loss 0.0106
[08:11:17]    Batch 280: Loss 0.0106
[08:11:33]    Batch 300: Loss 0.0106
[08:11:49]    Batch 320: Loss 0.0106
[08:12:06]    Batch 340: Loss 0.0106
[08:12:22]    Batch 360: Loss 0.0106
[08:12:38]    Batch 380: Loss 0.0106
[08:12:54]    Batch 400: Loss 0.0106
[08:13:10]    Batch 420: Loss 0.0106
[08:13:27]    Batch 440: Loss 0.0106
[08:13:43]    Batch 460: Loss 0.0106
[08:13:59]    Batch 480: Loss 0.0106
[08:14:15]    Batch 500: Loss 0.0106
[08:14:31]    Batch 520: Loss 0.0106
[08:14:48]    Batch 540: Loss 0.0106
[08:15:04]    Batch 560: Loss 0.0106
[08:15:20]    Batch 580: Loss 0.0106
[08:15:36]    Batch 600: Loss 0.0106
[08:15:52]    Batch 620: Loss 0.0107
[08:28:39] [Val Epoch 14] raw_min=0.0002 raw_max=0.9999 mode=dist->sim thr_score=0.650 thr_dist=0.358 f1=0.9301 auc=0.9714 acc=0.9300 edge_fb=0
[08:28:45] Epoch 14/50: loss=0.0107, val_loss=0.0198, acc=0.9654, val_acc=0.9300, val_f1=0.9301, val_auc=0.9714, thr=0.358, precision=0.9652, recall=0.9657 [1272.7s/epoch, ETA: 764.1min]
[08:28:54] Training completed in 297.2 minutes
[08:28:54] Loaded best checkpoint before final save: best_model_epoch_07_auc_0.9804_vloss_0.0333_vf1_0.9382.weights.h5 (val_auc=0.9804)
[08:29:05] Final model saved to: models\Proposed Siamese CapsNet + MobileNetV2_20251006_033126\models\final_best_model, models\Proposed Siamese CapsNet + MobileNetV2_20251006_033126\models\final_best_model.keras and weights models\Proposed Siamese CapsNet + MobileNetV2_20251006_033126\models\final_best_model.weights.h5
[08:29:05] Training completed successfully.