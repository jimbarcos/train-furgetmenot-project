[17:27:36] Data generators loaded from CSV files
[17:27:36] Welcome to Real-Time Model Training Interface!
[17:27:36] Select a model and configure parameters to begin training.
[17:27:49] Loading proposed model...
[17:28:00] Proposed Siamese CapsNet + MobileNetV2 loaded successfully.
[17:28:50] Reloading data generators with current batch size...
[17:29:01] Data generators loaded from CSV files
[17:29:01] Model directory created: models\Proposed Siamese CapsNet + MobileNetV2_20251006_172901
[17:29:01] Starting training for 50 epochs...
[17:29:01] Preparing model and data for training...
[17:29:03] [RoutingEntropy] mean_norm_entropy=0.9892
[17:29:05] Adaptive threshold mechanism enabled
[17:29:05] Batch sizes - Train: 32, Val: 16
[17:29:05] Data ready - Train: 625 batches, Val: 625 batches
[17:29:05] Training with learning rate: 5.00e-05
[17:29:05] Mixed precision training enabled
[17:29:05] Starting training with 50 epochs...
[17:29:05] Model parameters: 9,294,393
[17:29:05] [Pre-Train Distances] n=16 min=0.01512 max=0.06427 mean=0.03678 std=0.01585 spread=0.04915
[17:29:05] Models will be saved to: models\Proposed Siamese CapsNet + MobileNetV2_20251006_172901\models
[17:29:05] Training started - Preparing data...
[17:29:05] [ThresholdSync] Epoch 1: train threshold (dist) = 0.5000
[17:29:05] Starting Epoch 1...
[17:29:21]    Batch 0: Loss 1.3188
[17:29:53]    Batch 20: Loss 1.2816
[17:30:26]    Batch 40: Loss 1.2503
[17:30:56]    Batch 60: Loss 1.2209
[17:31:25]    Batch 80: Loss 1.1951
[17:31:57]    Batch 100: Loss 1.1720
[17:32:28]    Batch 120: Loss 1.1510
[17:33:02]    Batch 140: Loss 1.1314
[17:33:34]    Batch 160: Loss 1.1128
[17:34:06]    Batch 180: Loss 1.0951
[17:34:38]    Batch 200: Loss 1.0781
[17:35:09]    Batch 220: Loss 1.0617
[17:35:41]    Batch 240: Loss 1.0459
[17:36:13]    Batch 260: Loss 1.0306
[17:36:45]    Batch 280: Loss 1.0157
[17:37:15]    Batch 300: Loss 1.0012
[17:37:44]    Batch 320: Loss 0.9871
[17:38:14]    Batch 340: Loss 0.9733
[17:38:45]    Batch 360: Loss 0.9599
[17:39:16]    Batch 380: Loss 0.9468
[17:39:45]    Batch 400: Loss 0.9340
[17:40:14]    Batch 420: Loss 0.9215
[17:40:44]    Batch 440: Loss 0.9092
[17:41:13]    Batch 460: Loss 0.8973
[17:41:42]    Batch 480: Loss 0.8856
[17:42:12]    Batch 500: Loss 0.8741
[17:42:41]    Batch 520: Loss 0.8629
[17:43:11]    Batch 540: Loss 0.8520
[17:43:41]    Batch 560: Loss 0.8413
[17:44:11]    Batch 580: Loss 0.8308
[17:44:41]    Batch 600: Loss 0.8205
[17:45:10]    Batch 620: Loss 0.8105
[18:04:30] [Val Epoch 1] raw_min=0.0006 raw_max=0.9999 mode=dist->sim thr_score=0.620 thr_dist=0.452 f1=0.9025 auc=0.9647 acc=0.9020 edge_fb=0
[18:04:33] Epoch 1/50: loss=0.8085, val_loss=0.5060, acc=0.8163, val_acc=0.9020, val_f1=0.9025, val_auc=0.9647, thr=0.452, precision=0.8063, recall=0.8326 [2124.9s/epoch, ETA: 1735.4min]
[18:04:43] [ThresholdSync] Epoch 2: train threshold (dist) = 0.4521
[18:04:43] Starting Epoch 2...
[18:04:43]    Batch 0: Loss 0.4981
[18:05:06]    Batch 20: Loss 0.4912
[18:05:35]    Batch 40: Loss 0.4841
[18:06:05]    Batch 60: Loss 0.4777
[18:06:34]    Batch 80: Loss 0.4716
[18:07:04]    Batch 100: Loss 0.4656
[18:07:35]    Batch 120: Loss 0.4595
[18:08:08]    Batch 140: Loss 0.4536
[18:08:40]    Batch 160: Loss 0.4479
[18:09:12]    Batch 180: Loss 0.4423
[18:09:44]    Batch 200: Loss 0.4367
[18:10:17]    Batch 220: Loss 0.4313
[18:10:48]    Batch 240: Loss 0.4259
[18:11:19]    Batch 260: Loss 0.4207
[18:11:50]    Batch 280: Loss 0.4155
[18:12:21]    Batch 300: Loss 0.4104
[18:12:51]    Batch 320: Loss 0.4054
[18:13:23]    Batch 340: Loss 0.4006
[18:13:53]    Batch 360: Loss 0.3958
[18:14:24]    Batch 380: Loss 0.3911
[18:14:55]    Batch 400: Loss 0.3865
[18:15:25]    Batch 420: Loss 0.3820
[18:15:55]    Batch 440: Loss 0.3775
[18:16:25]    Batch 460: Loss 0.3732
[18:16:53]    Batch 480: Loss 0.3689
[18:17:22]    Batch 500: Loss 0.3647
[18:17:51]    Batch 520: Loss 0.3606
[18:18:20]    Batch 540: Loss 0.3565
[18:18:49]    Batch 560: Loss 0.3526
[18:19:18]    Batch 580: Loss 0.3486
[18:19:47]    Batch 600: Loss 0.3448
[18:20:17]    Batch 620: Loss 0.3410
[18:39:24] [Val Epoch 2] raw_min=0.0005 raw_max=0.9999 mode=dist->sim thr_score=0.510 thr_dist=0.467 f1=0.9187 auc=0.9642 acc=0.9185 edge_fb=0
[18:39:29] Epoch 2/50: loss=0.3403, val_loss=0.2321, acc=0.9108, val_acc=0.9185, val_f1=0.9187, val_auc=0.9642, thr=0.467, precision=0.9180, recall=0.9023 [2089.0s/epoch, ETA: 1687.6min]
[18:39:41] [ThresholdSync] Epoch 3: train threshold (dist) = 0.4674
[18:39:41] Starting Epoch 3...
[18:39:41]    Batch 0: Loss 0.2225
[18:39:59]    Batch 20: Loss 0.2212
[18:40:28]    Batch 40: Loss 0.2186
[18:40:57]    Batch 60: Loss 0.2159
[18:41:26]    Batch 80: Loss 0.2133
[18:41:55]    Batch 100: Loss 0.2107
[18:42:23]    Batch 120: Loss 0.2083
[18:42:52]    Batch 140: Loss 0.2060
[18:43:21]    Batch 160: Loss 0.2036
[18:43:51]    Batch 180: Loss 0.2013
[18:44:20]    Batch 200: Loss 0.1991
[18:44:51]    Batch 220: Loss 0.1968
[18:45:22]    Batch 240: Loss 0.1946
[18:45:53]    Batch 260: Loss 0.1925
[18:46:23]    Batch 280: Loss 0.1904
[18:46:55]    Batch 300: Loss 0.1883
[18:47:26]    Batch 320: Loss 0.1863
[18:47:57]    Batch 340: Loss 0.1843
[18:48:28]    Batch 360: Loss 0.1823
[18:48:58]    Batch 380: Loss 0.1804
[18:49:28]    Batch 400: Loss 0.1785
[18:49:57]    Batch 420: Loss 0.1766
[18:50:27]    Batch 440: Loss 0.1747
[18:50:56]    Batch 460: Loss 0.1730
[18:51:26]    Batch 480: Loss 0.1712
[18:51:55]    Batch 500: Loss 0.1694
[18:52:24]    Batch 520: Loss 0.1677
[18:52:54]    Batch 540: Loss 0.1661
[18:53:23]    Batch 560: Loss 0.1644
[18:53:54]    Batch 580: Loss 0.1628
[18:54:23]    Batch 600: Loss 0.1612
[18:54:53]    Batch 620: Loss 0.1597
[19:14:36] [Val Epoch 3] raw_min=0.0003 raw_max=0.9999 mode=dist->sim thr_score=0.490 thr_dist=0.484 f1=0.9378 auc=0.9773 acc=0.9375 edge_fb=0
[19:14:40] Epoch 3/50: loss=0.1594, val_loss=0.1182, acc=0.9315, val_acc=0.9375, val_f1=0.9378, val_auc=0.9773, thr=0.484, precision=0.9321, recall=0.9308 [2109.1s/epoch, ETA: 1653.0min]
[19:14:50] [ThresholdSync] Epoch 4: train threshold (dist) = 0.4845
[19:14:50] Starting Epoch 4...
[19:14:50]    Batch 0: Loss 0.1093
[19:15:14]    Batch 20: Loss 0.1096
[19:15:43]    Batch 40: Loss 0.1083
[19:16:14]    Batch 60: Loss 0.1073
[19:16:44]    Batch 80: Loss 0.1063
[19:17:15]    Batch 100: Loss 0.1051
[19:17:47]    Batch 120: Loss 0.1041
[19:18:18]    Batch 140: Loss 0.1031
[19:18:49]    Batch 160: Loss 0.1021
[19:19:19]    Batch 180: Loss 0.1012
[19:19:49]    Batch 200: Loss 0.1002
[19:20:19]    Batch 220: Loss 0.0992
[19:20:49]    Batch 240: Loss 0.0983
[19:21:19]    Batch 260: Loss 0.0974
[19:21:49]    Batch 280: Loss 0.0965
[19:22:19]    Batch 300: Loss 0.0957
[19:22:49]    Batch 320: Loss 0.0948
[19:23:21]    Batch 340: Loss 0.0940
[19:23:53]    Batch 360: Loss 0.0931
[19:24:25]    Batch 380: Loss 0.0922
[19:24:57]    Batch 400: Loss 0.0914
[19:25:29]    Batch 420: Loss 0.0906
[19:25:59]    Batch 440: Loss 0.0898
[19:26:29]    Batch 460: Loss 0.0891
[19:26:59]    Batch 480: Loss 0.0883
[19:27:29]    Batch 500: Loss 0.0875
[19:27:59]    Batch 520: Loss 0.0868
[19:28:29]    Batch 540: Loss 0.0861
[19:28:58]    Batch 560: Loss 0.0854
[19:29:27]    Batch 580: Loss 0.0847
[19:29:58]    Batch 600: Loss 0.0840
[19:30:30]    Batch 620: Loss 0.0833
[19:49:51] [Val Epoch 4] raw_min=0.0002 raw_max=0.9999 mode=dist->sim thr_score=0.570 thr_dist=0.463 f1=0.9416 auc=0.9770 acc=0.9415 edge_fb=0
[19:49:58] Epoch 4/50: loss=0.0832, val_loss=0.0699, acc=0.9439, val_acc=0.9415, val_f1=0.9416, val_auc=0.9770, thr=0.463, precision=0.9413, recall=0.9469 [2110.3s/epoch, ETA: 1618.9min]
[19:50:13] [ThresholdSync] Epoch 5: train threshold (dist) = 0.4627
[19:50:13] Starting Epoch 5...
[19:50:13]    Batch 0: Loss 0.0632
[19:50:28]    Batch 20: Loss 0.0619
[19:50:57]    Batch 40: Loss 0.0612
[19:51:27]    Batch 60: Loss 0.0607
[19:51:56]    Batch 80: Loss 0.0602
[19:52:25]    Batch 100: Loss 0.0596
[19:52:55]    Batch 120: Loss 0.0592
[19:53:24]    Batch 140: Loss 0.0588
[19:53:53]    Batch 160: Loss 0.0582
[19:54:23]    Batch 180: Loss 0.0578
[19:54:53]    Batch 200: Loss 0.0574
[19:55:23]    Batch 220: Loss 0.0571
[19:55:52]    Batch 240: Loss 0.0567
[19:56:22]    Batch 260: Loss 0.0562
[19:56:52]    Batch 280: Loss 0.0558
[19:57:21]    Batch 300: Loss 0.0554
[19:57:51]    Batch 320: Loss 0.0550
[19:58:20]    Batch 340: Loss 0.0546
[19:58:50]    Batch 360: Loss 0.0542
[19:59:19]    Batch 380: Loss 0.0539
[19:59:48]    Batch 400: Loss 0.0535
[20:00:17]    Batch 420: Loss 0.0531
[20:00:48]    Batch 440: Loss 0.0528
[20:01:20]    Batch 460: Loss 0.0524
[20:01:53]    Batch 480: Loss 0.0521
[20:02:25]    Batch 500: Loss 0.0518
[20:02:57]    Batch 520: Loss 0.0514
[20:03:27]    Batch 540: Loss 0.0511
[20:03:56]    Batch 560: Loss 0.0507
[20:04:26]    Batch 580: Loss 0.0504
[20:04:55]    Batch 600: Loss 0.0501
[20:05:25]    Batch 620: Loss 0.0498
[20:25:10] [Val Epoch 5] raw_min=0.0001 raw_max=0.9999 mode=dist->sim thr_score=0.550 thr_dist=0.458 f1=0.9443 auc=0.9729 acc=0.9440 edge_fb=0
[20:25:20] Epoch 5/50: loss=0.0497, val_loss=0.0479, acc=0.9462, val_acc=0.9440, val_f1=0.9443, val_auc=0.9729, thr=0.458, precision=0.9428, recall=0.9499 [2116.2s/epoch, ETA: 1584.8min]
[20:25:35] [ThresholdSync] Epoch 6: train threshold (dist) = 0.4576
[20:25:35] Starting Epoch 6...
[20:25:35]    Batch 0: Loss 0.0397
[20:25:46]    Batch 20: Loss 0.0400
[20:26:17]    Batch 40: Loss 0.0395
[20:26:49]    Batch 60: Loss 0.0394
[20:27:19]    Batch 80: Loss 0.0390
[20:27:50]    Batch 100: Loss 0.0387
[20:28:20]    Batch 120: Loss 0.0384
[20:28:51]    Batch 140: Loss 0.0381
[20:29:21]    Batch 160: Loss 0.0379
[20:29:52]    Batch 180: Loss 0.0377
[20:30:23]    Batch 200: Loss 0.0374
[20:30:54]    Batch 220: Loss 0.0372
[20:31:25]    Batch 240: Loss 0.0370
[20:31:56]    Batch 260: Loss 0.0368
[20:32:28]    Batch 280: Loss 0.0366
[20:32:58]    Batch 300: Loss 0.0364
[20:33:28]    Batch 320: Loss 0.0362
[20:33:59]    Batch 340: Loss 0.0360
[20:34:29]    Batch 360: Loss 0.0358
[20:35:00]    Batch 380: Loss 0.0356
[20:35:30]    Batch 400: Loss 0.0354
[20:36:01]    Batch 420: Loss 0.0352
[20:36:32]    Batch 440: Loss 0.0351
[20:37:03]    Batch 460: Loss 0.0349
[20:37:33]    Batch 480: Loss 0.0347
[20:38:05]    Batch 500: Loss 0.0345
[20:38:36]    Batch 520: Loss 0.0344
[20:39:08]    Batch 540: Loss 0.0342
[20:39:39]    Batch 560: Loss 0.0340
[20:40:11]    Batch 580: Loss 0.0338
[20:40:43]    Batch 600: Loss 0.0337
[20:41:15]    Batch 620: Loss 0.0335
[21:01:06] [Val Epoch 6] raw_min=0.0001 raw_max=0.9999 mode=dist->sim thr_score=0.630 thr_dist=0.423 f1=0.9325 auc=0.9695 acc=0.9325 edge_fb=0
[21:01:12] Epoch 6/50: loss=0.0334, val_loss=0.0368, acc=0.9460, val_acc=0.9325, val_f1=0.9325, val_auc=0.9695, thr=0.423, precision=0.9410, recall=0.9517 [2152.8s/epoch, ETA: 1554.8min]
[21:01:22] [ThresholdSync] Epoch 7: train threshold (dist) = 0.4226
[21:01:23] Starting Epoch 7...
[21:01:23]    Batch 0: Loss 0.0279
[21:01:42]    Batch 20: Loss 0.0278
[21:02:13]    Batch 40: Loss 0.0278
[21:02:43]    Batch 60: Loss 0.0276
[21:03:13]    Batch 80: Loss 0.0275
[21:03:44]    Batch 100: Loss 0.0275
[21:04:14]    Batch 120: Loss 0.0273
[21:04:44]    Batch 140: Loss 0.0271
[21:05:15]    Batch 160: Loss 0.0270
[21:05:45]    Batch 180: Loss 0.0269
[21:06:15]    Batch 200: Loss 0.0267
[21:06:46]    Batch 220: Loss 0.0266
[21:07:17]    Batch 240: Loss 0.0265
[21:07:47]    Batch 260: Loss 0.0263
[21:08:18]    Batch 280: Loss 0.0262
[21:08:48]    Batch 300: Loss 0.0261
[21:09:18]    Batch 320: Loss 0.0260
[21:09:48]    Batch 340: Loss 0.0259
[21:10:19]    Batch 360: Loss 0.0258
[21:10:50]    Batch 380: Loss 0.0257
[21:11:20]    Batch 400: Loss 0.0256
[21:11:51]    Batch 420: Loss 0.0255
[21:12:21]    Batch 440: Loss 0.0254
[21:12:52]    Batch 460: Loss 0.0252
[21:13:23]    Batch 480: Loss 0.0251
[21:13:54]    Batch 500: Loss 0.0250
[21:14:24]    Batch 520: Loss 0.0249
[21:14:53]    Batch 540: Loss 0.0248
[21:15:23]    Batch 560: Loss 0.0248
[21:15:52]    Batch 580: Loss 0.0247
[21:16:22]    Batch 600: Loss 0.0246
[21:16:52]    Batch 620: Loss 0.0245
[21:36:49] [Val Epoch 7] raw_min=0.0001 raw_max=0.9999 mode=dist->sim thr_score=0.650 thr_dist=0.394 f1=0.9390 auc=0.9722 acc=0.9390 edge_fb=0
[21:36:58] Epoch 7/50: loss=0.0245, val_loss=0.0313, acc=0.9485, val_acc=0.9390, val_f1=0.9390, val_auc=0.9722, thr=0.394, precision=0.9406, recall=0.9576 [2140.2s/epoch, ETA: 1521.8min]
[21:37:15] [ThresholdSync] Epoch 8: train threshold (dist) = 0.3936
[21:37:15] Starting Epoch 8...
[21:37:15]    Batch 0: Loss 0.0216
[21:37:30]    Batch 20: Loss 0.0214
[21:38:02]    Batch 40: Loss 0.0213
[21:38:36]    Batch 60: Loss 0.0212
[21:39:10]    Batch 80: Loss 0.0211
[21:39:43]    Batch 100: Loss 0.0211
[21:40:16]    Batch 120: Loss 0.0210
[21:40:50]    Batch 140: Loss 0.0211
[21:41:25]    Batch 160: Loss 0.0211
[21:42:02]    Batch 180: Loss 0.0209
[21:42:36]    Batch 200: Loss 0.0209
[21:43:12]    Batch 220: Loss 0.0208
[21:43:47]    Batch 240: Loss 0.0207
[21:44:21]    Batch 260: Loss 0.0206
[21:44:56]    Batch 280: Loss 0.0205
[21:45:31]    Batch 300: Loss 0.0204
[21:46:05]    Batch 320: Loss 0.0204
[21:46:40]    Batch 340: Loss 0.0203
[21:47:17]    Batch 360: Loss 0.0202
[21:47:51]    Batch 380: Loss 0.0201
[21:48:22]    Batch 400: Loss 0.0200
[21:48:54]    Batch 420: Loss 0.0199
[21:49:25]    Batch 440: Loss 0.0199
[21:49:57]    Batch 460: Loss 0.0198
[21:50:27]    Batch 480: Loss 0.0198
[21:50:56]    Batch 500: Loss 0.0197
[21:51:29]    Batch 520: Loss 0.0196
[21:52:00]    Batch 540: Loss 0.0195
[21:52:31]    Batch 560: Loss 0.0195
[21:53:03]    Batch 580: Loss 0.0194
[21:53:34]    Batch 600: Loss 0.0194
[21:54:05]    Batch 620: Loss 0.0193
[22:14:51] [Val Epoch 8] raw_min=0.0001 raw_max=0.9999 mode=dist->sim thr_score=0.670 thr_dist=0.368 f1=0.9257 auc=0.9663 acc=0.9255 edge_fb=0
[22:14:59] Epoch 8/50: loss=0.0193, val_loss=0.0262, acc=0.9443, val_acc=0.9255, val_f1=0.9257, val_auc=0.9663, thr=0.368, precision=0.9348, recall=0.9552 [2278.8s/epoch, ETA: 1500.3min]
[22:15:10] [ThresholdSync] Epoch 9: train threshold (dist) = 0.3682
[22:15:10] Starting Epoch 9...
[22:15:10]    Batch 0: Loss 0.0151
[22:15:29]    Batch 20: Loss 0.0169
[22:15:59]    Batch 40: Loss 0.0171
[22:16:31]    Batch 60: Loss 0.0172
[22:17:05]    Batch 80: Loss 0.0169
[22:17:36]    Batch 100: Loss 0.0169
[22:18:07]    Batch 120: Loss 0.0169
[22:18:37]    Batch 140: Loss 0.0169
[22:19:08]    Batch 160: Loss 0.0168
[22:19:38]    Batch 180: Loss 0.0168
[22:20:08]    Batch 200: Loss 0.0168
[22:20:38]    Batch 220: Loss 0.0168
[22:21:08]    Batch 240: Loss 0.0167
[22:21:38]    Batch 260: Loss 0.0167
[22:22:08]    Batch 280: Loss 0.0166
[22:22:38]    Batch 300: Loss 0.0166
[22:23:09]    Batch 320: Loss 0.0166
[22:23:40]    Batch 340: Loss 0.0166
[22:24:12]    Batch 360: Loss 0.0165
[22:24:41]    Batch 380: Loss 0.0165
[22:25:10]    Batch 400: Loss 0.0165
[22:25:40]    Batch 420: Loss 0.0164
[22:26:10]    Batch 440: Loss 0.0163
[22:26:39]    Batch 460: Loss 0.0163
[22:27:09]    Batch 480: Loss 0.0162
[22:27:39]    Batch 500: Loss 0.0162
[22:28:09]    Batch 520: Loss 0.0161
[22:28:39]    Batch 540: Loss 0.0161
[22:29:08]    Batch 560: Loss 0.0160
[22:29:38]    Batch 580: Loss 0.0160
[22:30:08]    Batch 600: Loss 0.0159
[22:30:39]    Batch 620: Loss 0.0159
[22:50:52] [Val Epoch 9] raw_min=0.0001 raw_max=0.9999 mode=dist->sim thr_score=0.720 thr_dist=0.333 f1=0.9408 auc=0.9726 acc=0.9400 edge_fb=0
[22:50:59] Epoch 9/50: loss=0.0159, val_loss=0.0237, acc=0.9492, val_acc=0.9400, val_f1=0.9408, val_auc=0.9726, thr=0.333, precision=0.9378, recall=0.9621 [2156.7s/epoch, ETA: 1465.9min]
[22:51:17] [ThresholdSync] Epoch 10: train threshold (dist) = 0.3329
[22:51:17] Starting Epoch 10...
[22:51:17]    Batch 0: Loss 0.0136
[22:51:33]    Batch 20: Loss 0.0150
[22:52:04]    Batch 40: Loss 0.0145
[22:52:36]    Batch 60: Loss 0.0145
[22:53:08]    Batch 80: Loss 0.0144
[22:53:41]    Batch 100: Loss 0.0143
[22:54:15]    Batch 120: Loss 0.0143
[22:54:48]    Batch 140: Loss 0.0143
[22:55:21]    Batch 160: Loss 0.0142
[22:55:52]    Batch 180: Loss 0.0143
[22:56:22]    Batch 200: Loss 0.0143
[22:56:52]    Batch 220: Loss 0.0142
[22:57:23]    Batch 240: Loss 0.0142
[22:57:53]    Batch 260: Loss 0.0142
[22:58:24]    Batch 280: Loss 0.0142
[22:58:54]    Batch 300: Loss 0.0142
[22:59:27]    Batch 320: Loss 0.0142
[23:00:00]    Batch 340: Loss 0.0141
[23:00:33]    Batch 360: Loss 0.0141
[23:01:05]    Batch 380: Loss 0.0141
[23:01:36]    Batch 400: Loss 0.0141
[23:02:07]    Batch 420: Loss 0.0141
[23:02:38]    Batch 440: Loss 0.0141
[23:03:08]    Batch 460: Loss 0.0141
[23:03:38]    Batch 480: Loss 0.0141
[23:04:08]    Batch 500: Loss 0.0140
[23:04:38]    Batch 520: Loss 0.0140
[23:05:08]    Batch 540: Loss 0.0140
[23:05:38]    Batch 560: Loss 0.0139
[23:06:10]    Batch 580: Loss 0.0139
[23:06:39]    Batch 600: Loss 0.0139
[23:07:10]    Batch 620: Loss 0.0139
[23:27:00] [Val Epoch 10] raw_min=0.0001 raw_max=0.9999 mode=dist->sim thr_score=0.560 thr_dist=0.376 f1=0.9390 auc=0.9678 acc=0.9380 edge_fb=0
[23:27:04] Epoch 10/50: loss=0.0139, val_loss=0.0224, acc=0.9487, val_acc=0.9380, val_f1=0.9390, val_auc=0.9678, thr=0.376, precision=0.9362, recall=0.9630 [2165.6s/epoch, ETA: 1431.7min]
[23:27:10] Training completed in 358.0 minutes
[23:27:10] Loaded best checkpoint before final save: best_model_epoch_03_auc_0.9773_vloss_0.1182_vf1_0.9378.weights.h5 (val_auc=0.9773)
[23:27:31] Final model saved to: models\Proposed Siamese CapsNet + MobileNetV2_20251006_172901\models\final_best_model, models\Proposed Siamese CapsNet + MobileNetV2_20251006_172901\models\final_best_model.keras and weights models\Proposed Siamese CapsNet + MobileNetV2_20251006_172901\models\final_best_model.weights.h5
[23:27:31] Training completed successfully.

